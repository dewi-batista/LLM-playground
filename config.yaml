transformer:
  batch_size: 400
  d_model: 768
  dropout: 0.1
  early_stop_delta: 0.1
  early_stop_patience: 10
  eval_batches: 100
  eval_every: 1000
  grad_accum_steps: 1
  grad_clip: 1.0
  log_every: 50
  lr: 2.5e-4
  min_count: 5
  num_blocks: 12
  seq_len: 128
  train_tokens: 1e10
  val_frac: 0.1
  warmup_frac: 0.02
  weight_decay: 0.1
