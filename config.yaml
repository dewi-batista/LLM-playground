transformer:
  batch_size: 375
  d_model: 768
  dropout: 0.1
  early_stop_delta: 0.1
  early_stop_patience: 10
  eval_batches: 10 # 50
  eval_every: 100 # 1000
  grad_accum_steps: 1
  grad_clip: 1.0
  lr: 2.5e-4
  min_count: 5
  num_blocks: 12
  seq_len: 128
  train_tokens: 1e8
  val_frac: 0.1
  warmup_frac: 0.02
  weight_decay: 0.1
