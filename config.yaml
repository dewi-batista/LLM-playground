transformer:
  batch_size: 375
  d_model: 768
  dropout: 0.1
  eval_batches: 100
  eval_every: 500
  grad_accum_steps: 1
  grad_clip: 1.0
  log_every: 50
  lr: 2.5e-4
  min_count: 5
  num_blocks: 12
  seq_len: 128
  train_tokens: 5e9
  val_frac: 0.1
  warmup_frac: 0.02
  weight_decay: 0.1
