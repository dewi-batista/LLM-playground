1. Change name structure, e.g. ./models/wiki103/20251231183907/vocab.json inbstead. Current naming is too much for the eyes to scan.
2. Make a directory for each model (each ckpt) and save training info inside.

# Reading
- warmup + cosine LR schedules
- gradient accumulation (multiple forward/backward passes before stepping)
- gradient clipping (stabilises training)
- warmup + cosine LR schedules
- AdamW + weight decay (and why you often exclude biases/norms)
- why head dim is often ~64 (so n_heads â‰ˆ d_model/64)
- splitting params into decay/no-decay groups (biases + norms usually no decay)
- AdamW defaults (betas/eps) used for transformers
- TF32 and why it can be a big speedup on Ampere+ GPUs
- fused AdamW
- PyTorch SDPA / FlashAttention kernels
- torch.compile (and why it can speed up steady-state training)