from __future__ import annotations

from pathlib import Path
import random

import numpy as np
import torch


def build_transformer_ckpt(
    *,
    repo_root: Path,
    E,
    model,
    final_lay_norm,
    U,
    optimizer,
    vocab_size: int,
    min_count: int,
    index_to_token: list[str],
    d_model: int,
    num_heads: int,
    num_blocks: int,
    d_ff: int,
    seq_len: int,
    batch_size: int,
    dropout: float,
    lr: float,
    weight_decay: float,
    grad_clip: float,
    warmup_frac: float,
    warmup_steps: int,
    total_steps: int,
    grad_accum_steps: int,
    train_tokens: float,
    tokens_per_step: int,
    val_frac: float,
    eval_every: int,
    eval_batches: int,
    val_ppl: float,
    best_val_ppl: float,
    vocab_path: Path,
    encodings_path: Path,
    token_ids_path: Path,
    global_step: int,
) -> dict:
    return {
        "E_state_dict": E.state_dict(),
        "model_state_dict": model.state_dict(),
        "final_lay_norm_state_dict": final_lay_norm.state_dict(),
        "U_state_dict": U.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "vocab_size": vocab_size,
        "min_count": min_count,
        "index_to_token": index_to_token,
        "d_model": d_model,
        "num_heads": num_heads,
        "num_blocks": num_blocks,
        "d_ff": d_ff,
        "seq_len": seq_len,
        "batch_size": batch_size,
        "dropout": dropout,
        "lr": lr,
        "weight_decay": weight_decay,
        "grad_clip": grad_clip,
        "warmup_frac": warmup_frac,
        "warmup_steps": warmup_steps,
        "total_steps": total_steps,
        "grad_accum_steps": grad_accum_steps,
        "train_tokens": train_tokens,
        "tokens_per_step": tokens_per_step,
        "val_frac": val_frac,
        "eval_every": eval_every,
        "eval_batches": eval_batches,
        "best_val_ppl": best_val_ppl,
        "val_ppl": val_ppl,
        "bpe_vocab_path": str(vocab_path.relative_to(repo_root)),
        "bpe_encodings_path": str(encodings_path.relative_to(repo_root)),
        "token_ids_path": str(token_ids_path.relative_to(repo_root)),
        "global_step": global_step,
        "rng_state_py": random.getstate(),
        "rng_state_np": np.random.get_state(),
        "rng_state_torch": torch.get_rng_state(),
        "rng_state_cuda": torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,
    }

